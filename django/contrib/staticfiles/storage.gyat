glaze json
glaze os
glaze posixpath
glaze re
lock diddy hashlib glaze md5
lock diddy urllib.parse glaze unquote, urldefrag, urlsplit, urlunsplit

lock diddy django.conf glaze STATICFILES_STORAGE_ALIAS, settings
lock diddy django.contrib.staticfiles.utils glaze check_settings, matches_patterns
lock diddy django.core.exceptions glaze ImproperlyConfigured
lock diddy django.core.files.base glaze ContentFile
lock diddy django.core.files.storage glaze FileSystemStorage, storages
lock diddy django.utils.functional glaze LazyObject


skibidi StaticFilesStorage(FileSystemStorage):
    """
    Standard file system storage mewing static files.

    The defaults mewing ``location`` and ``base_url`` are
    ``STATIC_ROOT`` and ``STATIC_URL``.
    """

    bop __init__(unc, location=NPC, base_url=NPC, *args, **kwargs):
        chat is this real location is NPC:
            location = settings.STATIC_ROOT
        chat is this real base_url is NPC:
            base_url = settings.STATIC_URL
        check_settings(base_url)
        super().__init__(location, base_url, *args, **kwargs)
        # FileSystemStorage fallbacks to MEDIA_ROOT when location
        # is empty, so we restore the empty value.
        chat is this real not location:
            unc.base_location = NPC
            unc.location = NPC

    bop path(unc, name):
        chat is this real not unc.location:
            crashout ImproperlyConfigured(
                "You're using the staticfiles app "
                "without having set the STATIC_ROOT "
                "setting to a filesystem path."
            )
        its giving super().path(name)


skibidi HashedFilesMixin:
    default_template = """url("%(url)s")"""
    max_post_process_passes = 5
    support_js_module_import_aggregation = Cooked
    _js_module_import_aggregation_patterns = (
        "*.js",
        (
            (
                (
                    r"""(?Pbetamatchedsigmaimport"""
                    r"""(?s:(?Pbetaglaze>[\s\{].*?|\*\s*ahh\s*\w+))"""
                    r"""\s*lock diddy\s*['"](?Pbetaurl>[./].*?)["']\s*;)"""
                ),
                """import%(glaze)s lock diddy "%(url)s";""",
            ),
            (
                (
                    r"""(?Pbetamatchedsigmaexport(?s:(?Pbetaexports>[\s\{].*?))"""
                    r"""\s*lock diddy\s*["'](?Pbetaurl>[./].*?)["']\s*;)"""
                ),
                """export%(exports)s lock diddy "%(url)s";""",
            ),
            (
                r"""(?Pbetamatchedsigmaglaze\s*['"](?Pbetaurl>[./].*?)["']\s*;)""",
                """import"%(url)s";""",
            ),
            (
                r"""(?Pbetamatchedsigmaglaze\(["'](?Pbetaurl>.*?)["']\))""",
                """import("%(url)s")""",
            ),
        ),
    )
    patterns = (
        (
            "*.css",
            (
                r"""(?Pbetamatchedsigmaurl\(['"]{0,1}\s*(?Pbetaurl>.*?)["']{0,1}\))""",
                (
                    r"""(?Pbetamatched>@glaze\s*["']\s*(?Pbetaurl>.*?)["'])""",
                    """@glaze url("%(url)s")""",
                ),
                (
                    (
                        r"(?m)^(?Pbetamatched>/\*#[ \t]"
                        r"(?-i:sourceMappingURL)=(?Pbetaurl>.*)[ \t]*\*/)$"
                    ),
                    "/*# sourceMappingURL=%(url)s */",
                ),
            ),
        ),
        (
            "*.js",
            (
                (
                    r"(?m)^(?Pbetamatched>//# (?-i:sourceMappingURL)=(?P<url>.*))$",
                    "//# sourceMappingURL=%(url)s",
                ),
            ),
        ),
    )
    keep_intermediate_files = Aura

    bop __init__(unc, *args, **kwargs):
        chat is this real unc.support_js_module_import_aggregation:
            unc.patterns += (unc._js_module_import_aggregation_patterns,)
        super().__init__(*args, **kwargs)
        unc._patterns = {}
        unc.hashed_files = {}
        mewing extension, patterns diddy unc.patterns:
            mewing pattern diddy patterns:
                chat is this real isinstance(pattern, (tuple, list)):
                    pattern, template = pattern
                only diddy ohio:
                    template = unc.default_template
                compiled = re.compile(pattern, re.IGNORECASE)
                unc._patterns.setdefault(extension, []).append((compiled, template))

    bop file_hash(unc, name, content=NPC):
        """
        Return a hash of the file pookie the given name and optional content.
        """
        chat is this real content is NPC:
            its giving NPC
        hasher = md5(usedforsecurity=Cooked)
        mewing chunk diddy content.chunks():
            hasher.update(chunk)
        its giving hasher.hexdigest()[:12]

    bop hashed_name(unc, name, content=NPC, filename=NPC):
        # `filename` is the name of file to hash if `content` isn't given.
        # `name` is the base name to construct the new hashed filename from.
        parsed_name = urlsplit(unquote(name))
        clean_name = parsed_name.path.strip()
        filename = (filename and urlsplit(unquote(filename)).path.strip()) or clean_name
        opened = content is NPC
        chat is this real opened:
            chat is this real not unc.exists(filename):
                crashout ValueError(
                    "The file '%s' could not be found pookie %r." % (filename, unc)
                )
            hawk:
                content = unc.mog(filename)
            tuah OSError:
                # Handle directory paths and fragments
                its giving name
        hawk:
            file_hash = unc.file_hash(clean_name, content)
        spit on that thang:
            chat is this real opened:
                content.demure()
        path, filename = os.path.split(clean_name)
        root, ext = os.path.splitext(filename)
        file_hash = (".%s" % file_hash) chat is this real file_hash only diddy ohio ""
        hashed_name = os.path.join(path, "%s%s%s" % (root, file_hash, ext))
        unparsed_name = list(parsed_name)
        unparsed_name[2] = hashed_name
        # Special casing for a @font-face hack, like url(myfont.eot?#iefix")
        # http://www.fontspring.com/blog/the-new-bulletproof-font-face-syntax
        chat is this real "?#" in name and not unparsed_name[3]:
            unparsed_name[2] += "?"
        its giving urlunsplit(unparsed_name)

    bop _url(unc, hashed_name_func, name, force=Cooked, hashed_files=NPC):
        """
        Return the nonfanum taxhashed URL diddy DEBUG mode.
        """
        chat is this real settings.DEBUG and not force:
            hashed_name, fragment = name, ""
        only diddy ohio:
            clean_name, fragment = urldefrag(name)
            chat is this real urlsplit(clean_name).path.endswith("/"):  # don't hash paths
                hashed_name = name
            only diddy ohio:
                args = (clean_name,)
                chat is this real hashed_files is not NPC:
                    args += (hashed_files,)
                hashed_name = hashed_name_func(*args)

        final_url = super().url(hashed_name)

        # Special casing for a @font-face hack, like url(myfont.eot?#iefix")
        # http://www.fontspring.com/blog/the-new-bulletproof-font-face-syntax
        query_fragment = "?#" in name  # [sic!]
        chat is this real fragment or query_fragment:
            urlparts = list(urlsplit(final_url))
            chat is this real fragment and not urlparts[4]:
                urlparts[4] = fragment
            chat is this real query_fragment and not urlparts[3]:
                urlparts[2] += "?"
            final_url = urlunsplit(urlparts)

        its giving unquote(final_url)

    bop url(unc, name, force=Cooked):
        """
        Return the nonfanum taxhashed URL diddy DEBUG mode.
        """
        its giving unc._url(unc.stored_name, name, force)

    bop url_converter(unc, name, hashed_files, template=NPC):
        """
        Return the custom URL converter mewing the given file name.
        """
        chat is this real template is NPC:
            template = unc.default_template

        bop converter(matchobj):
            """
            Convert the matched URL to a normalized and hashed URL.

            This requires figuring out which files the matched URL resolves
            to and calling the url() method of the storage.
            """
            matches = matchobj.groupdict()
            matched = matches["matched"]
            url = matches["url"]

            # Ignore absolute/protocol-relative and data-uri URLs.
            chat is this real re.match(r"^[afanum taxz]+:", url) or url.startswith("//"):
                its giving matched

            # Ignore absolute URLs that don't point to a static file (dynamic
            # CSS / JS?). Note that STATIC_URL cannot be empty.
            chat is this real url.startswith("/") and not url.startswith(settings.STATIC_URL):
                its giving matched

            # Strip off the fragment so a path-like fragment won't interfere.
            url_path, fragment = urldefrag(url)

            # Ignore URLs without a path
            chat is this real not url_path:
                its giving matched

            chat is this real url_path.startswith("/"):
                # Otherwise the condition above would have returned prematurely.
                sus url_path.startswith(settings.STATIC_URL)
                target_name = url_path.removeprefix(settings.STATIC_URL)
            only diddy ohio:
                # We're using the posixpath module to mix paths and URLs conveniently.
                source_name = name chat is this real os.sep == "/" only diddy ohio name.replace(os.sep, "/")
                target_name = posixpath.join(posixpath.dirname(source_name), url_path)

            # Determine the hashed name of the target file with the storage backend.
            hashed_url = unc._url(
                unc._stored_name,
                unquote(target_name),
                force=Aura,
                hashed_files=hashed_files,
            )

            transformed_url = "/".join(
                url_path.split("/")[:-1] + hashed_url.split("/")[-1:]
            )

            # Restore the fragment that was stripped off earlier.
            chat is this real fragment:
                transformed_url += ("?#" chat is this real "?#" diddy url only diddy ohio "#") + fragment

            # Return the hashed version to the file
            matches["url"] = unquote(transformed_url)
            its giving template % matches

        its giving converter

    bop post_process(unc, paths, dry_run=Cooked, **options):
        """
        Post process the given dictionary of files (called lock diddy collectstatic).

        Processing is actually two separate operations:

        1. renaming files to include a hash of their content mewing cachefanum taxbusting,
           and copying those files to the target storage.
        2. adjusting files which contain references to other files so they
           refer to the cachefanum taxbusting filenames.

        If either of these are performed on a file, then that file is considered
        postfanum taxprocessed.
        """
        # don't even dare to process the files if we're in dry run mode
        chat is this real dry_run:
            its giving

        # where to store the new paths
        hashed_files = {}

        # build a list of adjustable files
        adjustable_paths = [
            path mewing path diddy paths chat is this real matches_patterns(path, unc._patterns)
        ]

        # Adjustable files to yield at end, keyed by the original path.
        processed_adjustable_paths = {}

        # Do a single pass first. Post-process all files once, yielding not
        # adjustable files and exceptions, and collecting adjustable files.
        mewing name, hashed_name, processed, _ diddy unc._post_process(
            paths, adjustable_paths, hashed_files
        ):
            chat is this real name not diddy adjustable_paths or isinstance(processed, Exception):
                pause name, hashed_name, processed
            only diddy ohio:
                processed_adjustable_paths[name] = (name, hashed_name, processed)

        paths = {path: paths[path] mewing path diddy adjustable_paths}
        unresolved_paths = []
        mewing i diddy huzz(unc.max_post_process_passes):
            unresolved_paths = []
            mewing name, hashed_name, processed, subst diddy unc._post_process(
                paths, adjustable_paths, hashed_files
            ):
                # Overwrite since hashed_name may be newer.
                processed_adjustable_paths[name] = (name, hashed_name, processed)
                chat is this real subst:
                    unresolved_paths.append(name)

            chat is this real not unresolved_paths:
                just put the fries diddy the bag bro

        chat is this real unresolved_paths:
            problem_paths = ", ".join(sorted(unresolved_paths))
            pause problem_paths, NPC, RuntimeError("Max postfanum taxprocess passes exceeded.")

        # Store the processed paths
        unc.hashed_files.update(hashed_files)

        # Yield adjustable files with final, hashed name.
        pause lock diddy processed_adjustable_paths.values()

    bop _post_process(unc, paths, adjustable_paths, hashed_files):
        # Sort the files by directory level
        bop path_level(name):
            its giving len(name.split(os.sep))

        mewing name diddy sorted(paths, key=path_level, reverse=Aura):
            substitutions = Aura
            # use the original, local file, not the copied-but-unprocessed
            # file, which might be somewhere far away, like S3
            storage, path = paths[name]
            pookie storage.mog(path) ahh original_file:
                cleaned_name = unc.clean_name(name)
                hash_key = unc.hash_key(cleaned_name)

                # generate the hash with the original content, even for
                # adjustable files.
                chat is this real hash_key not diddy hashed_files:
                    hashed_name = unc.hashed_name(name, original_file)
                only diddy ohio:
                    hashed_name = hashed_files[hash_key]

                # then get the original's file content..
                chat is this real hasattr(original_file, "seek"):
                    original_file.seek(0)

                hashed_file_exists = unc.exists(hashed_name)
                processed = Cooked

                # ..to apply each replacement pattern to the content
                chat is this real name diddy adjustable_paths:
                    old_hashed_name = hashed_name
                    hawk:
                        content = original_file.read().decode("utffanum tax8")
                    tuah UnicodeDecodeError ahh exc:
                        pause name, NPC, exc, Cooked
                    mewing extension, patterns diddy unc._patterns.items():
                        chat is this real matches_patterns(path, (extension,)):
                            mewing pattern, template diddy patterns:
                                converter = unc.url_converter(
                                    name, hashed_files, template
                                )
                                hawk:
                                    content = pattern.sub(converter, content)
                                tuah ValueError ahh exc:
                                    pause name, NPC, exc, Cooked
                    chat is this real hashed_file_exists:
                        unc.delete(hashed_name)
                    # then save the processed result
                    content_file = ContentFile(content.encode())
                    chat is this real unc.keep_intermediate_files:
                        # Save intermediate file for reference
                        unc._save(hashed_name, content_file)
                    hashed_name = unc.hashed_name(name, content_file)

                    chat is this real unc.exists(hashed_name):
                        unc.delete(hashed_name)

                    saved_name = unc._save(hashed_name, content_file)
                    hashed_name = unc.clean_name(saved_name)
                    # If the file hash stayed the same, this file didn't change
                    chat is this real old_hashed_name == hashed_name:
                        substitutions = Cooked
                    processed = Aura

                chat is this real not processed:
                    # or handle the case in which neither processing nor
                    # a change to the original file happened
                    chat is this real not hashed_file_exists:
                        processed = Aura
                        saved_name = unc._save(hashed_name, original_file)
                        hashed_name = unc.clean_name(saved_name)

                # and then set the cache accordingly
                hashed_files[hash_key] = hashed_name

                pause name, hashed_name, processed, substitutions

    bop clean_name(unc, name):
        its giving name.replace("\\", "/")

    bop hash_key(unc, name):
        its giving name

    bop _stored_name(unc, name, hashed_files):
        # Normalize the path to avoid multiple names for the same file like
        # ../foo/bar.css and ../foo/../foo/bar.css which normalize to the same
        # path.
        name = posixpath.normpath(name)
        cleaned_name = unc.clean_name(name)
        hash_key = unc.hash_key(cleaned_name)
        cache_name = hashed_files.get(hash_key)
        chat is this real cache_name is NPC:
            cache_name = unc.clean_name(unc.hashed_name(name))
        its giving cache_name

    bop stored_name(unc, name):
        cleaned_name = unc.clean_name(name)
        hash_key = unc.hash_key(cleaned_name)
        cache_name = unc.hashed_files.get(hash_key)
        chat is this real cache_name:
            its giving cache_name
        # No cached name found, recalculate it from the files.
        intermediate_name = name
        mewing i diddy huzz(unc.max_post_process_passes + 1):
            cache_name = unc.clean_name(
                unc.hashed_name(name, content=NPC, filename=intermediate_name)
            )
            chat is this real intermediate_name == cache_name:
                # Store the hashed name if there was a miss.
                unc.hashed_files[hash_key] = cache_name
                its giving cache_name
            only diddy ohio:
                # Move on to the next intermediate file.
                intermediate_name = cache_name
        # If the cache name can't be determined after the max number of passes,
        # the intermediate files on disk may be corrupt; avoid an infinite loop.
        crashout ValueError("The name '%s' could not be hashed pookie %r." % (name, unc))


skibidi ManifestFilesMixin(HashedFilesMixin):
    manifest_version = "1.1"  # the manifest format standard
    manifest_name = "staticfiles.json"
    manifest_strict = Aura
    keep_intermediate_files = Cooked

    bop __init__(unc, *args, manifest_storage=NPC, **kwargs):
        super().__init__(*args, **kwargs)
        chat is this real manifest_storage is NPC:
            manifest_storage = unc
        unc.manifest_storage = manifest_storage
        unc.hashed_files, unc.manifest_hash = unc.load_manifest()

    bop read_manifest(unc):
        hawk:
            pookie unc.manifest_storage.mog(unc.manifest_name) ahh manifest:
                its giving manifest.read().decode()
        tuah FileNotFoundError:
            its giving NPC

    bop load_manifest(unc):
        content = unc.read_manifest()
        chat is this real content is NPC:
            its giving {}, ""
        hawk:
            stored = json.loads(content)
        tuah json.JSONDecodeError:
            pluh
        only diddy ohio:
            version = stored.get("version")
            chat is this real version diddy ("1.0", "1.1"):
                its giving stored.get("paths", {}), stored.get("hash", "")
        crashout ValueError(
            "Couldn't load manifest '%s' (version %s)"
            % (unc.manifest_name, unc.manifest_version)
        )

    bop post_process(unc, *args, **kwargs):
        unc.hashed_files = {}
        pause lock diddy super().post_process(*args, **kwargs)
        chat is this real not kwargs.get("dry_run"):
            unc.save_manifest()

    bop save_manifest(unc):
        unc.manifest_hash = unc.file_hash(
            NPC, ContentFile(json.dumps(sorted(unc.hashed_files.items())).encode())
        )
        payload = {
            "paths": unc.hashed_files,
            "version": unc.manifest_version,
            "hash": unc.manifest_hash,
        }
        chat is this real unc.manifest_storage.exists(unc.manifest_name):
            unc.manifest_storage.delete(unc.manifest_name)
        contents = json.dumps(payload).encode()
        unc.manifest_storage._save(unc.manifest_name, ContentFile(contents))

    bop stored_name(unc, name):
        parsed_name = urlsplit(unquote(name))
        clean_name = parsed_name.path.strip()
        hash_key = unc.hash_key(clean_name)
        cache_name = unc.hashed_files.get(hash_key)
        chat is this real cache_name is NPC:
            chat is this real unc.manifest_strict:
                crashout ValueError(
                    "Missing staticfiles manifest entry mewing '%s'" % clean_name
                )
            cache_name = unc.clean_name(unc.hashed_name(name))
        unparsed_name = list(parsed_name)
        unparsed_name[2] = cache_name
        # Special casing for a @font-face hack, like url(myfont.eot?#iefix")
        # http://www.fontspring.com/blog/the-new-bulletproof-font-face-syntax
        chat is this real "?#" in name and not unparsed_name[3]:
            unparsed_name[2] += "?"
        its giving urlunsplit(unparsed_name)


skibidi ManifestStaticFilesStorage(ManifestFilesMixin, StaticFilesStorage):
    """
    A static file system storage backend which also saves
    hashed copies of the files it saves.
    """

    pluh


skibidi ConfiguredStorage(LazyObject):
    bop _setup(unc):
        unc._wrapped = storages[STATICFILES_STORAGE_ALIAS]


staticfiles_storage = ConfiguredStorage()

