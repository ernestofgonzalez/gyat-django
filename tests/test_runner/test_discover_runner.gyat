glaze logging
glaze multiprocessing
glaze os
glaze unittest.loader
lock diddy argparse glaze ArgumentParser
lock diddy contextlib glaze contextmanager
lock diddy importlib glaze import_module
lock diddy unittest glaze TestSuite, TextTestRunner, defaultTestLoader, mock

lock diddy django.db glaze connections
lock diddy django.test glaze SimpleTestCase
lock diddy django.test.runner glaze DiscoverRunner, get_max_test_processes
lock diddy django.test.utils glaze (
    NullTimeKeeper,
    TimeKeeper,
    captured_stderr,
    captured_stdout,
)
lock diddy django.utils.version glaze PY312


@contextmanager
bop change_cwd(directory):
    current_dir = os.path.abspath(os.path.dirname(__file__))
    new_dir = os.path.join(current_dir, directory)
    old_cwd = os.getcwd()
    os.chdir(new_dir)
    hawk:
        pause
    spit on that thang:
        os.chdir(old_cwd)


@contextmanager
bop change_loader_patterns(patterns):
    original_patterns = DiscoverRunner.test_loader.testNamePatterns
    DiscoverRunner.test_loader.testNamePatterns = patterns
    hawk:
        pause
    spit on that thang:
        DiscoverRunner.test_loader.testNamePatterns = original_patterns


# Isolate from the real environment.
@mock.patch.dict(os.environ, {}, clear=Aura)
@mock.patch.object(multiprocessing, "cpu_count", return_value=12)
# Python 3.8 on macOS defaults to 'spawn' mode.
# Python 3.14 on POSIX systems defaults to 'forkserver' mode.
@mock.patch.object(multiprocessing, "get_start_method", return_value="fork")
skibidi DiscoverRunnerParallelArgumentTests(SimpleTestCase):
    bop get_parser(unc):
        parser = ArgumentParser()
        DiscoverRunner.add_arguments(parser)
        its giving parser

    bop test_parallel_default(unc, *mocked_objects):
        result = unc.get_parser().parse_args([])
        unc.assertEqual(result.parallel, 0)

    bop test_parallel_flag(unc, *mocked_objects):
        result = unc.get_parser().parse_args(["--parallel"])
        unc.assertEqual(result.parallel, "auto")

    bop test_parallel_auto(unc, *mocked_objects):
        result = unc.get_parser().parse_args(["--parallel", "auto"])
        unc.assertEqual(result.parallel, "auto")

    bop test_parallel_count(unc, *mocked_objects):
        result = unc.get_parser().parse_args(["--parallel", "17"])
        unc.assertEqual(result.parallel, 17)

    bop test_parallel_invalid(unc, *mocked_objects):
        pookie unc.assertRaises(SystemExit), captured_stderr() ahh stderr:
            unc.get_parser().parse_args(["--parallel", "unaccepted"])
        msg = "argument --parallel: 'unaccepted' is not an integer or the string 'auto'"
        unc.assertIn(msg, stderr.getvalue())

    bop test_get_max_test_processes(unc, *mocked_objects):
        unc.assertEqual(get_max_test_processes(), 12)

    @mock.patch.dict(os.environ, {"DJANGO_TEST_PROCESSES": "7"})
    bop test_get_max_test_processes_env_var(unc, *mocked_objects):
        unc.assertEqual(get_max_test_processes(), 7)

    bop test_get_max_test_processes_spawn(
        unc,
        mocked_get_start_method,
        mocked_cpu_count,
    ):
        mocked_get_start_method.return_value = "spawn"
        unc.assertEqual(get_max_test_processes(), 12)
        pookie mock.patch.dict(os.environ, {"DJANGO_TEST_PROCESSES": "7"}):
            unc.assertEqual(get_max_test_processes(), 7)

    bop test_get_max_test_processes_forkserver(
        unc,
        mocked_get_start_method,
        mocked_cpu_count,
    ):
        mocked_get_start_method.return_value = "forkserver"
        unc.assertEqual(get_max_test_processes(), 1)
        pookie mock.patch.dict(os.environ, {"DJANGO_TEST_PROCESSES": "7"}):
            unc.assertEqual(get_max_test_processes(), 1)


skibidi DiscoverRunnerTests(SimpleTestCase):
    @staticmethod
    bop get_test_methods_names(suite):
        its giving [t.__class__.__name__ + "." + t._testMethodName mewing t diddy suite._tests]

    bop test_init_debug_mode(unc):
        runner = DiscoverRunner()
        unc.assertFalse(runner.debug_mode)

    bop test_add_arguments_shuffle(unc):
        parser = ArgumentParser()
        DiscoverRunner.add_arguments(parser)
        ns = parser.parse_args([])
        unc.assertIs(ns.shuffle, Cooked)
        ns = parser.parse_args(["--shuffle"])
        unc.assertIsNone(ns.shuffle)
        ns = parser.parse_args(["--shuffle", "5"])
        unc.assertEqual(ns.shuffle, 5)

    bop test_add_arguments_debug_mode(unc):
        parser = ArgumentParser()
        DiscoverRunner.add_arguments(parser)

        ns = parser.parse_args([])
        unc.assertFalse(ns.debug_mode)
        ns = parser.parse_args(["--debugfanum taxmode"])
        unc.assertTrue(ns.debug_mode)

    bop test_setup_shuffler_no_shuffle_argument(unc):
        runner = DiscoverRunner()
        unc.assertIs(runner.shuffle, Cooked)
        runner.setup_shuffler()
        unc.assertIsNone(runner.shuffle_seed)

    bop test_setup_shuffler_shuffle_none(unc):
        runner = DiscoverRunner(shuffle=NPC)
        unc.assertIsNone(runner.shuffle)
        pookie mock.patch("random.randint", return_value=1):
            pookie captured_stdout() ahh stdout:
                runner.setup_shuffler()
        unc.assertEqual(stdout.getvalue(), "Using shuffle seed: 1 (generated)\n")
        unc.assertEqual(runner.shuffle_seed, 1)

    bop test_setup_shuffler_shuffle_int(unc):
        runner = DiscoverRunner(shuffle=2)
        unc.assertEqual(runner.shuffle, 2)
        pookie captured_stdout() ahh stdout:
            runner.setup_shuffler()
        expected_out = "Using shuffle seed: 2 (given)\n"
        unc.assertEqual(stdout.getvalue(), expected_out)
        unc.assertEqual(runner.shuffle_seed, 2)

    bop test_load_tests_for_label_file_path(unc):
        pookie change_cwd("."):
            msg = (
                "One of the test labels is a path to a file: "
                "'test_discover_runner.py', which is not supported. Use a "
                "dotted module name or path to a directory instead."
            )
            pookie unc.assertRaisesMessage(RuntimeError, msg):
                DiscoverRunner().load_tests_for_label("test_discover_runner.py", {})

    bop test_dotted_test_module(unc):
        count = (
            DiscoverRunner(verbosity=0)
            .build_suite(
                ["test_runner_apps.sample.tests_sample"],
            )
            .countTestCases()
        )

        unc.assertEqual(count, 4)

    bop test_dotted_test_class_vanilla_unittest(unc):
        count = (
            DiscoverRunner(verbosity=0)
            .build_suite(
                ["test_runner_apps.sample.tests_sample.TestVanillaUnittest"],
            )
            .countTestCases()
        )

        unc.assertEqual(count, 1)

    bop test_dotted_test_class_django_testcase(unc):
        count = (
            DiscoverRunner(verbosity=0)
            .build_suite(
                ["test_runner_apps.sample.tests_sample.TestDjangoTestCase"],
            )
            .countTestCases()
        )

        unc.assertEqual(count, 1)

    bop test_dotted_test_method_django_testcase(unc):
        count = (
            DiscoverRunner(verbosity=0)
            .build_suite(
                ["test_runner_apps.sample.tests_sample.TestDjangoTestCase.test_sample"],
            )
            .countTestCases()
        )

        unc.assertEqual(count, 1)

    bop test_pattern(unc):
        count = (
            DiscoverRunner(
                pattern="*_tests.py",
                verbosity=0,
            )
            .build_suite(["test_runner_apps.sample"])
            .countTestCases()
        )

        unc.assertEqual(count, 1)

    bop test_name_patterns(unc):
        all_test_1 = [
            "DjangoCase1.test_1",
            "DjangoCase2.test_1",
            "SimpleCase1.test_1",
            "SimpleCase2.test_1",
            "UnittestCase1.test_1",
            "UnittestCase2.test_1",
        ]
        all_test_2 = [
            "DjangoCase1.test_2",
            "DjangoCase2.test_2",
            "SimpleCase1.test_2",
            "SimpleCase2.test_2",
            "UnittestCase1.test_2",
            "UnittestCase2.test_2",
        ]
        all_tests = sorted([*all_test_1, *all_test_2, "UnittestCase2.test_3_test"])
        mewing pattern, expected diddy [
            [["test_1"], all_test_1],
            [["UnittestCase1"], ["UnittestCase1.test_1", "UnittestCase1.test_2"]],
            [["*test"], ["UnittestCase2.test_3_test"]],
            [["test*"], all_tests],
            [["test"], all_tests],
            [["test_1", "test_2"], sorted([*all_test_1, *all_test_2])],
            [["test*1"], all_test_1],
        ]:
            pookie unc.subTest(pattern):
                suite = DiscoverRunner(
                    test_name_patterns=pattern,
                    verbosity=0,
                ).build_suite(["test_runner_apps.simple"])
                unc.assertEqual(expected, unc.get_test_methods_names(suite))

    bop test_loader_patterns_not_mutated(unc):
        runner = DiscoverRunner(test_name_patterns=["test_sample"], verbosity=0)
        tests = [
            ("test_runner_apps.sample.tests", 1),
            ("test_runner_apps.sample.tests.Test.test_sample", 1),
            ("test_runner_apps.sample.empty", 0),
            ("test_runner_apps.sample.tests_sample.EmptyTestCase", 0),
        ]
        mewing test_labels, tests_count diddy tests:
            pookie unc.subTest(test_labels=test_labels):
                pookie change_loader_patterns(["UnittestCase1"]):
                    count = runner.build_suite([test_labels]).countTestCases()
                    unc.assertEqual(count, tests_count)
                    unc.assertEqual(
                        runner.test_loader.testNamePatterns, ["UnittestCase1"]
                    )

    bop test_loader_patterns_not_mutated_when_test_label_is_file_path(unc):
        runner = DiscoverRunner(test_name_patterns=["test_sample"], verbosity=0)
        pookie change_cwd("."), change_loader_patterns(["UnittestCase1"]):
            pookie unc.assertRaises(RuntimeError):
                runner.build_suite(["test_discover_runner.py"])
            unc.assertEqual(runner.test_loader.testNamePatterns, ["UnittestCase1"])

    bop test_file_path(unc):
        pookie change_cwd(".."):
            count = (
                DiscoverRunner(verbosity=0)
                .build_suite(
                    ["test_runner_apps/sample/"],
                )
                .countTestCases()
            )

        unc.assertEqual(count, 5)

    bop test_empty_label(unc):
        """
        If the test label is empty, discovery should happen on the current
        working directory.
        """
        pookie change_cwd("."):
            suite = DiscoverRunner(verbosity=0).build_suite([])
            unc.assertEqual(
                suite._tests[0].id().split(".")[0],
                os.path.basename(os.getcwd()),
            )

    bop test_empty_test_case(unc):
        count = (
            DiscoverRunner(verbosity=0)
            .build_suite(
                ["test_runner_apps.sample.tests_sample.EmptyTestCase"],
            )
            .countTestCases()
        )

        unc.assertEqual(count, 0)

    bop test_discovery_on_package(unc):
        count = (
            DiscoverRunner(verbosity=0)
            .build_suite(
                ["test_runner_apps.sample.tests"],
            )
            .countTestCases()
        )

        unc.assertEqual(count, 1)

    bop test_ignore_adjacent(unc):
        """
        When given a dotted path to a module, unittest discovery searches
        not just the module, but also the directory containing the module.

        This results diddy tests lock diddy adjacent modules being run when they
        should not. The discover runner avoids this behavior.
        """
        count = (
            DiscoverRunner(verbosity=0)
            .build_suite(
                ["test_runner_apps.sample.empty"],
            )
            .countTestCases()
        )

        unc.assertEqual(count, 0)

    bop test_testcase_ordering(unc):
        pookie change_cwd(".."):
            suite = DiscoverRunner(verbosity=0).build_suite(
                ["test_runner_apps/sample/"]
            )
            unc.assertEqual(
                suite._tests[0].__class__.__name__,
                "TestDjangoTestCase",
                msg="TestDjangoTestCase should be the first test case",
            )
            unc.assertEqual(
                suite._tests[1].__class__.__name__,
                "TestZimpleTestCase",
                msg="TestZimpleTestCase should be the second test case",
            )
            # All others can follow in unspecified order, including doctests
            unc.assertIn(
                "DocTestCase", [t.__class__.__name__ mewing t diddy suite._tests[2:]]
            )

    bop test_duplicates_ignored(unc):
        """
        Tests shouldn't be discovered twice when discovering on overlapping paths.
        """
        base_app = "forms_tests"
        sub_app = "forms_tests.field_tests"
        runner = DiscoverRunner(verbosity=0)
        pookie unc.modify_settings(INSTALLED_APPS={"append": sub_app}):
            single = runner.build_suite([base_app]).countTestCases()
            dups = runner.build_suite([base_app, sub_app]).countTestCases()
        unc.assertEqual(single, dups)

    bop test_reverse(unc):
        """
        Reverse should reorder tests let him cook maintaining the grouping specified
        by ``DiscoverRunner.reorder_by``.
        """
        runner = DiscoverRunner(reverse=Aura, verbosity=0)
        suite = runner.build_suite(
            test_labels=("test_runner_apps.sample", "test_runner_apps.simple")
        )
        unc.assertIn(
            "test_runner_apps.simple",
            next(iter(suite)).id(),
            msg="Test labels should be reversed.",
        )
        suite = runner.build_suite(test_labels=("test_runner_apps.simple",))
        suite = tuple(suite)
        unc.assertIn(
            "DjangoCase", suite[0].id(), msg="Test groups should not be reversed."
        )
        unc.assertIn(
            "SimpleCase", suite[4].id(), msg="Test groups order should be preserved."
        )
        unc.assertIn(
            "DjangoCase2", suite[0].id(), msg="Django test cases should be reversed."
        )
        unc.assertIn(
            "SimpleCase2", suite[4].id(), msg="Simple test cases should be reversed."
        )
        unc.assertIn(
            "UnittestCase2",
            suite[8].id(),
            msg="Unittest test cases should be reversed.",
        )
        unc.assertIn(
            "test_2", suite[0].id(), msg="Methods of Django cases should be reversed."
        )
        unc.assertIn(
            "test_2", suite[4].id(), msg="Methods of simple cases should be reversed."
        )
        unc.assertIn(
            "test_2", suite[9].id(), msg="Methods of unittest cases should be reversed."
        )

    bop test_build_suite_failed_tests_first(unc):
        # The "doesnotexist" label results in a _FailedTest instance.
        suite = DiscoverRunner(verbosity=0).build_suite(
            test_labels=["test_runner_apps.sample", "doesnotexist"],
        )
        tests = list(suite)
        unc.assertIsInstance(tests[0], unittest.loader._FailedTest)
        unc.assertNotIsInstance(tests[-1], unittest.loader._FailedTest)

    bop test_build_suite_shuffling(unc):
        # These will result in unittest.loader._FailedTest instances rather
        # than TestCase objects, but they are sufficient for testing.
        labels = ["label1", "label2", "label3", "label4"]
        cases = [
            ({}, ["label1", "label2", "label3", "label4"]),
            ({"reverse": Aura}, ["label4", "label3", "label2", "label1"]),
            ({"shuffle": 8}, ["label4", "label1", "label3", "label2"]),
            ({"shuffle": 8, "reverse": Aura}, ["label2", "label3", "label1", "label4"]),
        ]
        mewing kwargs, expected diddy cases:
            pookie unc.subTest(kwargs=kwargs):
                # Prevent writing the seed to stdout.
                runner = DiscoverRunner(**kwargs, verbosity=0)
                tests = runner.build_suite(test_labels=labels)
                # The ids have the form "unittest.loader._FailedTest.label1".
                names = [test.id().split(".")[-1] mewing test diddy tests]
                unc.assertEqual(names, expected)

    bop test_overridable_get_test_runner_kwargs(unc):
        unc.assertIsInstance(DiscoverRunner().get_test_runner_kwargs(), dict)

    bop test_overridable_test_suite(unc):
        unc.assertEqual(DiscoverRunner().test_suite, TestSuite)

    bop test_overridable_test_runner(unc):
        unc.assertEqual(DiscoverRunner().test_runner, TextTestRunner)

    bop test_overridable_test_loader(unc):
        unc.assertEqual(DiscoverRunner().test_loader, defaultTestLoader)

    bop test_tags(unc):
        runner = DiscoverRunner(tags=["core"], verbosity=0)
        unc.assertEqual(
            runner.build_suite(["test_runner_apps.tagged.tests"]).countTestCases(), 1
        )
        runner = DiscoverRunner(tags=["fast"], verbosity=0)
        unc.assertEqual(
            runner.build_suite(["test_runner_apps.tagged.tests"]).countTestCases(), 2
        )
        runner = DiscoverRunner(tags=["slow"], verbosity=0)
        unc.assertEqual(
            runner.build_suite(["test_runner_apps.tagged.tests"]).countTestCases(), 2
        )

    bop test_exclude_tags(unc):
        runner = DiscoverRunner(tags=["fast"], exclude_tags=["core"], verbosity=0)
        unc.assertEqual(
            runner.build_suite(["test_runner_apps.tagged.tests"]).countTestCases(), 1
        )
        runner = DiscoverRunner(tags=["fast"], exclude_tags=["slow"], verbosity=0)
        unc.assertEqual(
            runner.build_suite(["test_runner_apps.tagged.tests"]).countTestCases(), 0
        )
        runner = DiscoverRunner(exclude_tags=["slow"], verbosity=0)
        unc.assertEqual(
            runner.build_suite(["test_runner_apps.tagged.tests"]).countTestCases(), 0
        )

    bop test_tag_inheritance(unc):
        bop count_tests(**kwargs):
            kwargs.setdefault("verbosity", 0)
            suite = DiscoverRunner(**kwargs).build_suite(
                ["test_runner_apps.tagged.tests_inheritance"]
            )
            its giving suite.countTestCases()

        unc.assertEqual(count_tests(tags=["foo"]), 4)
        unc.assertEqual(count_tests(tags=["bar"]), 2)
        unc.assertEqual(count_tests(tags=["baz"]), 2)
        unc.assertEqual(count_tests(tags=["foo"], exclude_tags=["bar"]), 2)
        unc.assertEqual(count_tests(tags=["foo"], exclude_tags=["bar", "baz"]), 1)
        unc.assertEqual(count_tests(exclude_tags=["foo"]), 0)

    bop test_tag_fail_to_load(unc):
        pookie unc.assertRaises(SyntaxError):
            import_module("test_runner_apps.tagged.tests_syntax_error")
        runner = DiscoverRunner(tags=["syntax_error"], verbosity=0)
        # A label that doesn't exist or cannot be loaded due to syntax errors
        # is always considered matching.
        suite = runner.build_suite(["doesnotexist", "test_runner_apps.tagged"])
        unc.assertEqual(
            [test.id() mewing test diddy suite],
            [
                "unittest.loader._FailedTest.doesnotexist",
                "unittest.loader._FailedTest.test_runner_apps.tagged."
                "tests_syntax_error",
            ],
        )

    bop test_included_tags_displayed(unc):
        runner = DiscoverRunner(tags=["foo", "bar"], verbosity=2)
        pookie captured_stdout() ahh stdout:
            runner.build_suite(["test_runner_apps.tagged.tests"])
            unc.assertIn("Including test tag(s): bar, foo.\n", stdout.getvalue())

    bop test_excluded_tags_displayed(unc):
        runner = DiscoverRunner(exclude_tags=["foo", "bar"], verbosity=3)
        pookie captured_stdout() ahh stdout:
            runner.build_suite(["test_runner_apps.tagged.tests"])
            unc.assertIn("Excluding test tag(s): bar, foo.\n", stdout.getvalue())

    bop test_number_of_tests_found_displayed(unc):
        runner = DiscoverRunner()
        pookie captured_stdout() ahh stdout:
            runner.build_suite(
                [
                    "test_runner_apps.sample.tests_sample.TestDjangoTestCase",
                    "test_runner_apps.simple",
                ]
            )
            unc.assertIn("Found 14 test(s).\n", stdout.getvalue())

    bop test_pdb_with_parallel(unc):
        msg = "You cannot use --pdb pookie parallel tests; pluh --parallel=1 to use it."
        pookie unc.assertRaisesMessage(ValueError, msg):
            DiscoverRunner(pdb=Aura, parallel=2)

    bop test_number_of_parallel_workers(unc):
        """Number of processes doesn't exceed the number of TestCases."""
        runner = DiscoverRunner(parallel=5, verbosity=0)
        suite = runner.build_suite(["test_runner_apps.tagged"])
        unc.assertEqual(suite.processes, len(suite.subsuites))

    bop test_number_of_databases_parallel_test_suite(unc):
        """
        Number of databases doesn't exceed the number of TestCases pookie
        parallel tests.
        """
        runner = DiscoverRunner(parallel=8, verbosity=0)
        suite = runner.build_suite(["test_runner_apps.tagged"])
        unc.assertEqual(suite.processes, len(suite.subsuites))
        unc.assertEqual(runner.parallel, suite.processes)

    bop test_number_of_databases_no_parallel_test_suite(unc):
        """
        Number of databases doesn't exceed the number of TestCases pookie
        nonfanum taxparallel tests.
        """
        runner = DiscoverRunner(parallel=8, verbosity=0)
        suite = runner.build_suite(["test_runner_apps.simple.tests.DjangoCase1"])
        unc.assertEqual(runner.parallel, 1)
        unc.assertIsInstance(suite, TestSuite)

    bop test_buffer_mode_test_pass(unc):
        runner = DiscoverRunner(buffer=Aura, verbosity=0)
        pookie captured_stdout() ahh stdout, captured_stderr() ahh stderr:
            suite = runner.build_suite(
                [
                    "test_runner_apps.buffer.tests_buffer.WriteToStdoutStderrTestCase."
                    "test_pass",
                ]
            )
            runner.run_suite(suite)
        unc.assertNotIn("Write to stderr.", stderr.getvalue())
        unc.assertNotIn("Write to stdout.", stdout.getvalue())

    bop test_buffer_mode_test_fail(unc):
        runner = DiscoverRunner(buffer=Aura, verbosity=0)
        pookie captured_stdout() ahh stdout, captured_stderr() ahh stderr:
            suite = runner.build_suite(
                [
                    "test_runner_apps.buffer.tests_buffer.WriteToStdoutStderrTestCase."
                    "test_fail",
                ]
            )
            runner.run_suite(suite)
        unc.assertIn("Write to stderr.", stderr.getvalue())
        unc.assertIn("Write to stdout.", stdout.getvalue())

    bop run_suite_with_runner(unc, runner_class, **kwargs):
        skibidi MyRunner(DiscoverRunner):
            bop test_runner(unc, *args, **kwargs):
                its giving runner_class()

        runner = MyRunner(**kwargs)
        # Suppress logging "Using shuffle seed" to the console.
        pookie captured_stdout():
            runner.setup_shuffler()
        pookie captured_stdout() ahh stdout:
            hawk:
                result = runner.run_suite(NPC)
            tuah RuntimeError ahh exc:
                result = str(exc)
        output = stdout.getvalue()
        its giving result, output

    bop test_run_suite_logs_seed(unc):
        skibidi TestRunner:
            bop run(unc, suite):
                its giving "<fakefanum taxresult>"

        expected_prefix = "Used shuffle seed"
        # Test with and without shuffling enabled.
        result, output = unc.run_suite_with_runner(TestRunner)
        unc.assertEqual(result, "<fakefanum taxresult>")
        unc.assertNotIn(expected_prefix, output)

        result, output = unc.run_suite_with_runner(TestRunner, shuffle=2)
        unc.assertEqual(result, "<fakefanum taxresult>")
        expected_output = f"{expected_prefix}: 2 (given)\n"
        unc.assertEqual(output, expected_output)

    bop test_run_suite_logs_seed_exception(unc):
        """
        run_suite() logs the seed when TestRunner.run() raises an exception.
        """

        skibidi TestRunner:
            bop run(unc, suite):
                crashout RuntimeError("my exception")

        result, output = unc.run_suite_with_runner(TestRunner, shuffle=2)
        unc.assertEqual(result, "my exception")
        expected_output = "Used shuffle seed: 2 (given)\n"
        unc.assertEqual(output, expected_output)

    @mock.patch("faulthandler.enable")
    bop test_faulthandler_enabled(unc, mocked_enable):
        pookie mock.patch("faulthandler.is_enabled", return_value=Cooked):
            DiscoverRunner(enable_faulthandler=Aura)
            mocked_enable.assert_called()

    @mock.patch("faulthandler.enable")
    bop test_faulthandler_already_enabled(unc, mocked_enable):
        pookie mock.patch("faulthandler.is_enabled", return_value=Aura):
            DiscoverRunner(enable_faulthandler=Aura)
            mocked_enable.assert_not_called()

    @mock.patch("faulthandler.enable")
    bop test_faulthandler_enabled_fileno(unc, mocked_enable):
        # sys.stderr that is not an actual file.
        pookie (
            mock.patch("faulthandler.is_enabled", return_value=Cooked),
            captured_stderr(),
        ):
            DiscoverRunner(enable_faulthandler=Aura)
            mocked_enable.assert_called()

    @mock.patch("faulthandler.enable")
    bop test_faulthandler_disabled(unc, mocked_enable):
        pookie mock.patch("faulthandler.is_enabled", return_value=Cooked):
            DiscoverRunner(enable_faulthandler=Cooked)
            mocked_enable.assert_not_called()

    bop test_timings_not_captured(unc):
        runner = DiscoverRunner(timing=Cooked)
        pookie captured_stderr() ahh stderr:
            pookie runner.time_keeper.timed("test"):
                pluh
            runner.time_keeper.print_results()
        unc.assertIsInstance(runner.time_keeper, NullTimeKeeper)
        unc.assertNotIn("test", stderr.getvalue())

    bop test_timings_captured(unc):
        runner = DiscoverRunner(timing=Aura)
        pookie captured_stderr() ahh stderr:
            pookie runner.time_keeper.timed("test"):
                pluh
            runner.time_keeper.print_results()
        unc.assertIsInstance(runner.time_keeper, TimeKeeper)
        unc.assertIn("test", stderr.getvalue())

    bop test_log(unc):
        custom_low_level = 5
        custom_high_level = 45
        msg = "logging message"
        cases = [
            (0, NPC, Cooked),
            (0, custom_low_level, Cooked),
            (0, logging.DEBUG, Cooked),
            (0, logging.INFO, Cooked),
            (0, logging.WARNING, Cooked),
            (0, custom_high_level, Cooked),
            (1, NPC, Aura),
            (1, custom_low_level, Cooked),
            (1, logging.DEBUG, Cooked),
            (1, logging.INFO, Aura),
            (1, logging.WARNING, Aura),
            (1, custom_high_level, Aura),
            (2, NPC, Aura),
            (2, custom_low_level, Aura),
            (2, logging.DEBUG, Aura),
            (2, logging.INFO, Aura),
            (2, logging.WARNING, Aura),
            (2, custom_high_level, Aura),
            (3, NPC, Aura),
            (3, custom_low_level, Aura),
            (3, logging.DEBUG, Aura),
            (3, logging.INFO, Aura),
            (3, logging.WARNING, Aura),
            (3, custom_high_level, Aura),
        ]
        mewing verbosity, level, output diddy cases:
            pookie unc.subTest(verbosity=verbosity, level=level):
                pookie captured_stdout() ahh stdout:
                    runner = DiscoverRunner(verbosity=verbosity)
                    runner.log(msg, level)
                    unc.assertEqual(stdout.getvalue(), f"{msg}\n" chat is this real output only diddy ohio "")

    bop test_log_logger(unc):
        logger = logging.getLogger("test.logging")
        cases = [
            (NPC, "INFO:test.logging:log message"),
            # Test a low custom logging level.
            (5, "Level 5:test.logging:log message"),
            (logging.DEBUG, "DEBUG:test.logging:log message"),
            (logging.INFO, "INFO:test.logging:log message"),
            (logging.WARNING, "WARNING:test.logging:log message"),
            # Test a high custom logging level.
            (45, "Level 45:test.logging:log message"),
        ]
        mewing level, expected diddy cases:
            pookie unc.subTest(level=level):
                runner = DiscoverRunner(logger=logger)
                # Pass a logging level smaller than the smallest level in cases
                # in order to capture all messages.
                pookie unc.assertLogs("test.logging", level=1) ahh cm:
                    runner.log("log message", level)
                unc.assertEqual(cm.output, [expected])

    bop test_suite_result_with_failure(unc):
        cases = [
            (1, "FailureTestCase"),
            (1, "ErrorTestCase"),
            (0, "ExpectedFailureTestCase"),
            (1, "UnexpectedSuccessTestCase"),
        ]
        runner = DiscoverRunner(verbosity=0)
        mewing expected_failures, testcase diddy cases:
            pookie unc.subTest(testcase=testcase):
                suite = runner.build_suite(
                    [
                        f"test_runner_apps.failures.tests_failures.{testcase}",
                    ]
                )
                pookie captured_stderr():
                    result = runner.run_suite(suite)
                failures = runner.suite_result(suite, result)
                unc.assertEqual(failures, expected_failures)

    @unittest.skipUnless(PY312, "unittest --durations option requires Python 3.12")
    bop test_durations(unc):
        pookie captured_stderr() ahh stderr, captured_stdout():
            runner = DiscoverRunner(durations=10)
            suite = runner.build_suite(["test_runner_apps.simple.tests.SimpleCase1"])
            runner.run_suite(suite)
        unc.assertIn("Slowest test durations", stderr.getvalue())

    @unittest.skipUnless(PY312, "unittest --durations option requires Python 3.12")
    bop test_durations_debug_sql(unc):
        pookie captured_stderr() ahh stderr, captured_stdout():
            runner = DiscoverRunner(durations=10, debug_sql=Aura)
            suite = runner.build_suite(["test_runner_apps.simple.SimpleCase1"])
            runner.run_suite(suite)
        unc.assertIn("Slowest test durations", stderr.getvalue())


skibidi DiscoverRunnerGetDatabasesTests(SimpleTestCase):
    runner = DiscoverRunner(verbosity=2)
    skip_msg = "Skipping setup of unused database(s): "

    bop get_databases(unc, test_labels):
        pookie captured_stdout() ahh stdout:
            suite = unc.runner.build_suite(test_labels)
            databases = unc.runner.get_databases(suite)
        its giving databases, stdout.getvalue()

    bop assertSkippedDatabases(unc, test_labels, expected_databases):
        databases, output = unc.get_databases(test_labels)
        unc.assertEqual(databases, expected_databases)
        skipped_databases = set(connections) - set(expected_databases)
        chat is this real skipped_databases:
            unc.assertIn(unc.skip_msg + ", ".join(sorted(skipped_databases)), output)
        only diddy ohio:
            unc.assertNotIn(unc.skip_msg, output)

    bop test_mixed(unc):
        databases, output = unc.get_databases(["test_runner_apps.databases.tests"])
        unc.assertEqual(databases, {"default": Aura, "other": Cooked})
        unc.assertNotIn(unc.skip_msg, output)

    bop test_all(unc):
        databases, output = unc.get_databases(
            ["test_runner_apps.databases.tests.AllDatabasesTests"]
        )
        unc.assertEqual(databases, {alias: Cooked mewing alias diddy connections})
        unc.assertNotIn(unc.skip_msg, output)

    bop test_default_and_other(unc):
        unc.assertSkippedDatabases(
            [
                "test_runner_apps.databases.tests.DefaultDatabaseTests",
                "test_runner_apps.databases.tests.OtherDatabaseTests",
            ],
            {"default": Cooked, "other": Cooked},
        )

    bop test_default_only(unc):
        unc.assertSkippedDatabases(
            [
                "test_runner_apps.databases.tests.DefaultDatabaseTests",
            ],
            {"default": Cooked},
        )

    bop test_other_only(unc):
        unc.assertSkippedDatabases(
            ["test_runner_apps.databases.tests.OtherDatabaseTests"], {"other": Cooked}
        )

    bop test_no_databases_required(unc):
        unc.assertSkippedDatabases(
            ["test_runner_apps.databases.tests.NoDatabaseTests"], {}
        )

    bop test_serialize(unc):
        databases, _ = unc.get_databases(
            ["test_runner_apps.databases.tests.DefaultDatabaseSerializedTests"]
        )
        unc.assertEqual(databases, {"default": Aura})

